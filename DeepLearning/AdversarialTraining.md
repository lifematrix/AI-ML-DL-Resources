

## Papers

### 2018

* **Towards deep learning models resistant to adversarial attacks** (Madry et al; ICLR 2018) [[Paper]](https://arxiv.org/pdf/1706.06083.pdf)
  * The code of challenges: [CIFAR 10](https://github.com/MadryLab/cifar10_challenge), [MNIST](https://github.com/MadryLab/mnist_challenge)

* **Adversarial Attacks and Defences: A Survey** (Chakraborty et al.) [[Paper]](https://arxiv.org/pdf/1810.00069.pdf)

### 2017

* **Adversarial Machine Learning at Scale** (Kurakin, Bengio, et al; ICLR 2017; Google Brain; I-FGSM) [[Paper]](https://arxiv.org/pdf/1611.01236.pdf)

* **The Space of Transferable Adversarial Examples** (Tramèr, et al; 2017) [[Paper]](https://arxiv.org/pdf/1704.03453.pdf)

### 2015

* **Deep neural networks are easily fooled: High confidence predictions for unrecognizable images** (Nguyen et al; CVPR 2015) [[Paper]](https://arxiv.org/pdf/1412.1897.pdf)

### 2014

* **Explaining and Harnessing Adversarial Examples** (Goodfellow et al; Cited by 6.8k) [[Paper]](https://arxiv.org/pdf/1412.6572.pdf)
 * *the l∞-ball around x has recently been studied as a natural notion for adversarial perturbations in this paper.*

### 2013

* **Evasion Attacks against Machine Learning at Test Time** (Biggio et al; ECML-KDD 2013) [[Paper]](https://arxiv.org/pdf/1708.06131)

* **Intriguing properties of neural networks** (Szegedy et al; Cited by 6k) [[Paper]](https://arxiv.org/pdf/1312.6199.pdf)

### 2008

* **A discriminatively trained, multiscale, de-formable part model** (Felzenszwalb et al.; CVPR 2008) [[Paper]](

### 1945

* **Statistical decision functions which minimize the maximum risk** (Wald et al.; In Annals of Mathematics, 1945) [[Paper]](https://booksc.xyz/book/32516015/33b0de)

## Code

* CleverHans -- A Python library to benchmark machine learning system' vulnerability to adversarial example
  * [github](https://github.com/cleverhans-lab/cleverhans). Note: ver 4.0 Supports TF2 on python3.
