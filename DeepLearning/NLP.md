
## BERT


### Paper

* **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (Devlin et al) [[Paper]](https://arxiv.org/abs/1810.04805)

### Articles and blog 

* Jay Alammar's Posts

  * BERT -- [**The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)**](http://jalammar.github.io/illustrated-bert/)
  
  * Transformer -- [**The Illustrated Transformer**](http://jalammar.github.io/illustrated-transformer/)

  * Attention -- [**Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)**](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
  
* [**Tokenizers: How machines read**](https://blog.floydhub.com/tokenization-nlp/) by Cathal Horan

* [**Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing**](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) -- announcement of BERT.

* [**The Annotated Transformer**](http://nlp.seas.harvard.edu/2018/04/03/attention.html) -- a python notebook about Transformer.

### Dataset

* [**SQuAD 2.0: The standford question answering dataset**](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/)
