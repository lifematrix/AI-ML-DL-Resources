
## Papers 

### 2020

* **A Survey on Contextual Embeddings** (Liu et al.) [[Paper]](https://arxiv.org/pdf/2003.07278.pdf)
* **A Survey on Transfer Learning in Natural Language Processing** (Alyafeai et al.; 2020) [[Paper]](https://arxiv.org/pdf/2007.04239.pdf)

### 2019

* **Evolution of Transfer Learning in Natural Language Processing** (Malte et al.) [[Paper]](https://arxiv.org/pdf/1910.07370.pdf)

* **TENER: Adapting Transformer Encoder for Named Entity Recognition** (Yan et al.; 2019) [[Paper]](https://arxiv.org/pdf/1911.04474.pdf)

* **Well-Read Students Learn Better: On the Importance of Pre-training Compact Models** (Turc et al.) [[Paper]](https://arxiv.org/pdf/1908.08962.pdf)

### 2018

* **Deep contextualized word representations** (Peters et al.; 2018; Known as ELMo; Cited by 5517) [[Paper]](https://arxiv.org/pdf/1802.05365.pdf)
* **Universal Language Model Fine-tuning for Text Classification** (Howard et al.; ACL 2018; Known as **ULMFiT**) [[Paper]](https://arxiv.org/pdf/1801.06146.pdf)

* **Improving Language Understanding by Generative Pre-Training** (Radford et al.; Known as OpenAI Transformer; Cited by 1593) [[Paper]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

* **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (Devlin et al.;Cited by 14k) [[Paper]](https://arxiv.org/abs/1810.04805)
  * Code
    * [Pytorch and Tensorflow implementation](https://github.com/huggingface/transformers)
    * [BERT End to End (Fine-tuning + Predicting) with Cloud TPU](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb), it is a google official colab notebook using two TPUs.
    * [TensorFlow very 2.0 (Keras) implementation of BERT](https://github.com/kpe/bert-for-tf2)
    * [Hugging's Face Pytorch version](https://github.com/huggingface/transformers)
    
* **SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing** (Kuda, et al.; 2018) [[Paper]](https://arxiv.org/pdf/1808.06226.pdf)

### 2017

* **Attention Is All You Need** (Vaswani et al.; 2017; NISP 2017; Cited by 16k) [[Paper]](https://arxiv.org/pdf/1706.03762.pdf)
  * [**The Annotated Transformer**](https://nlp.seas.harvard.edu/2018/04/03/attention.html) in Harvardnlp.  A very great guide written by Vaswani, the author of this paper.
  * [NLP Turtorails](https://github.com/graykode/nlp-tutorial/tree/master/5-1.Transformer)
  * **A clean example**: [blog](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec), [github](https://github.com/SamLynnEvans/Transformer)

* **Convolutional Sequence to Sequence Learning** (Gehring et al.; ICML 2017) [[Paper]](https://arxiv.org/pdf/1705.03122.pdf)

* **Language Modeling with Gated Convolutional Networks** (N.Dauphin et al.; PMLR 2017) [[Paper]](https://arxiv.org/pdf/1612.08083.pdf)

### 2016

* **Neural Machine Translation in Linear Time** (Kalchbrenner et al.; 2016) [[Paper]](https://arxiv.org/pdf/1610.10099.pdf)

* **Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation** (Wu et al.; 2016) [[Paper]](https://arxiv.org/pdf/1609.08144.pdf)

### 2015

* **Semi-supervised Sequence Learning** (Dai et al.; NIPS 2015; Cited by 818) [[Paper]](https://arxiv.org/pdf/1511.01432.pdf)

* **Neural Machine Translation of Rare Words with Subword Units** (Sennrich et al.; ACL 2016; Byte-pair Representation) [[Paper]](https://arxiv.org/pdf/1508.07909.pdf)
  * [jupyter tutorial](https://ufal.mff.cuni.cz/~helcl/courses/npfl116/ipython/byte_pair_encoding.html)

* **CHRF: character n-gram F-score for automatic MT evaluation** (Maja Popovic ÃÅ; Proceedings of the Tenth Workshop on Statistical Machine Translation) [[Paper]](https://www.aclweb.org/anthology/W15-3049.pdf)
 
### 2011

* **Natural Language Processing (Almost) from Scratch** (Collobert, Ronan; JMLR 2011) [[Paper]](https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)
### 2001

* **Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies** (Hochreiter et al.; 2001) [[Paper]](https://ml.jku.at/publications/older/ch7.pdf)
 
### 1997

* **Long short-term memory** (Hochreiter et al. Neural computation; Cited by 42k) [[Paper]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf)

### 1992

* **An estimate of an upper bound for the entropy of English** (Peter F. Brown, et al. Computational Linguistics, Vol 18, Issue 1, March 1992, pp 31-40.) [[Paper]](https://www.aclweb.org/anthology/J92-1002.pdf)


## Articles, blogs and slides

### blog

* Jay Alammar's Posts

  * BERT -- [**The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)**](http://jalammar.github.io/illustrated-bert/)
  
  * Transformer -- [**The Illustrated Transformer**](http://jalammar.github.io/illustrated-transformer/)

  * Attention -- [**Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)**](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
  
  * [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://nlp.stanford.edu/seminar/details/jdevlin.pdf) by Jacob Devlin, Google AI language
  
* [**Tokenizers: How machines read**](https://blog.floydhub.com/tokenization-nlp/) by Cathal Horan

* [**Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing**](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) -- announcement of BERT.

* [**The Annotated Transformer**](http://nlp.seas.harvard.edu/2018/04/03/attention.html) -- a python notebook about Transformer.

* [A Review of Deep Contextualized Word Representations](https://www.slideshare.net/shuntaroy/a-review-of-deep-contextualized-word-representations-peters-2018) -- Slides by Peters (2018)

* [Attention and its Different Forms](https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc) by Anusha Lihala by Anusha Lihala

* [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) by Amirhossein Kazemnejad, 2019

* Tokenizer
  * [What is Byte-Pair Encoding for Tokenization?](https://rutumulkar.com/blog/2021/byte-pair-encoding/) by Rutu Mulkar
  * [3 subword algorithms help to improve your NLP model performance](https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46) by Edward Ma

### Slides

* [Natural Language Processing with Deep Learning CS224N/Ling284](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture12-subwords.pdf)

## Dataset

* [**SQuAD 2.0: The standford question answering dataset**](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/)

* [**XNLI: The Cross-Lingual NLI Corpus**](https://github.com/facebookresearch/XNLI) on github by Facebook Research

### Translation

* [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki)

* [25 Best Parallel Translations Data Sources for Machine Learning](https://lionbridge.ai/datasets/25-best-parallel-text-datasets-for-machine-translation-training/)


### Neural Machine Translation at Standford NLP Group

Source Web Page is <https://nlp.stanford.edu/projects/nmt/>

* Preprocessed Data
  * WMT'14 English-German data [Medium]

    * Train(4.5M Sentence Pairs): [train.en](https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en) [train.de](https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de) 
    
* [The crawl scripts of Bookcorpus on the github](https://github.com/soskek/bookcorpus)


## Tutorials

* [Fine-tuning a BERT model](https://www.tensorflow.org/official_models/fine_tuning_bert)


## Codes

* [OpenNMT-tf](https://github.com/OpenNMT/OpenNMT-tf)


## Conferences

* [International Workshop on Spoken Language Translation, December 4-5, 2024](https://workshop2014.iwslt.org/downloads/proceeding.pdf)


