
## BERT 

### 2020

* **A Survey on Contextual Embeddings** (Liu et al.) [[Paper]](https://arxiv.org/pdf/2003.07278.pdf)
* **A Survey on Transfer Learning in Natural Language Processing** (Alyafeai et al.; 2020) [[Paper]](https://arxiv.org/pdf/2007.04239.pdf)

### 2019

* **Evolution of Transfer Learning in Natural Language Processing** (Malte et al.) [[Paper]](https://arxiv.org/pdf/1910.07370.pdf)

### 2018

* **Deep contextualized word representations** (Peters et al.; 2018; Known as ELMo; Cited by 5517) [[Paper]](https://arxiv.org/pdf/1802.05365.pdf)
* **Universal Language Model Fine-tuning for Text Classification** (Howard et al.; ACL 2018; Known as **ULMFiT**) [[Paper]](https://arxiv.org/pdf/1801.06146.pdf)

* **Improving Language Understanding by Generative Pre-Training** (Radford et al.; Known as OpenAI Transformer; Cited by 1593) [[Paper]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

###

* **Attention Is All You Need** (Vaswani et al.; 2017; Cited by 16k) [[Paper]](https://arxiv.org/pdf/1706.03762.pdf)

### 2015

 * **Semi-supervised Sequence Learning** (Dai et al.; NIPS 2015; Cited by 818) [[Paper]](https://arxiv.org/pdf/1511.01432.pdf)
 
 
### Paper

* **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (Devlin et al) [[Paper]](https://arxiv.org/abs/1810.04805)

### Articles and blog 

* Jay Alammar's Posts

  * BERT -- [**The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)**](http://jalammar.github.io/illustrated-bert/)
  
  * Transformer -- [**The Illustrated Transformer**](http://jalammar.github.io/illustrated-transformer/)

  * Attention -- [**Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)**](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
  
* [**Tokenizers: How machines read**](https://blog.floydhub.com/tokenization-nlp/) by Cathal Horan

* [**Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing**](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) -- announcement of BERT.

* [**The Annotated Transformer**](http://nlp.seas.harvard.edu/2018/04/03/attention.html) -- a python notebook about Transformer.

* [A Review of Deep Contextualized Word Representations](https://www.slideshare.net/shuntaroy/a-review-of-deep-contextualized-word-representations-peters-2018) -- Slides by Peters (2018)

### Dataset

* [**SQuAD 2.0: The standford question answering dataset**](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/)
