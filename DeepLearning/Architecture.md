

## Papers

### 2021

* **Characterizing signal propagation to close the performance gap in unnormal- ized resnets** (Brock et al.; ICLR 2021) [[Paper]](https://arxiv.org/pdf/2101.08692.pdf)

* **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale** (Dosovitskiy et al.; ViT: Vision Transformer) [[Paper]](https://arxiv.org/pdf/2010.11929.pdf)
  * [code and pretrained model](https://github.com/google-research/vision_transformer)
  
* **Revisiting ResNets: Improved Training and Scaling Strategies** (Bello et al.; 2021) [[Paper]](https://arxiv.org/pdf/2103.07579.pdf)

* **MLP-Mixer: An all-MLP Architecture for Vision** (Tolstikhin et al.; 2021; Google Brain) [[Paper]](https://arxiv.org/pdf/2105.01601.pdf)

* **Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet** (Kyriazi et al.; 2021; Oxford VGG) [[Paper]](https://arxiv.org/pdf/2105.02723.pdf)

### 2020

* **Meta pseudo labels** (Pham et al.; 2020) [[Paper]](https://arxiv.org/pdf/2003.10580.pdf)

* **Batch normalization biases residual blocks towards the identity function in deep networks** (De et al.; NeurPS 2020) [[Paper]](https://arxiv.org/pdf/2002.10444.pdf)

* **Circumventing Outliers of AutoAugment with Knowledge Distillation** (Wei et al.) [[Paper]](https://arxiv.org/pdf/2003.11342v1.pdf)

* **Self-training with noisy student improves imagenet classification** (Xie et al.; CVPR 2020) [[Paper]](https://arxiv.org/pdf/1911.04252.pdf)

* **Exploring self-attention for image recognition** (Zhao et al.; CVPR 2020) [[Paper]](https://arxiv.org/pdf/2004.13621.pdf)

### 2019

* **Micro-Batch Training with Batch-Channel Normalization and Weight Standardization** (Qiao et al.) [[Paper]](https://arxiv.org/pdf/1903.10520.pdf)

* **A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark** (Zhai et al.) [[Paper]](https://arxiv.org/pdf/1910.04867.pdf)

* **Data-efficient image recognition with contrastive predictive coding** (Hénaff et al.; ECCV 2020) [[Paper]](https://arxiv.org/pdf/1905.09272.pdf)


### 2018

* **Exploring the limits of weakly supervised pretraining** (Mahajan, He, et al; ECCV 2018) [[Paper]](https://arxiv.org/pdf/1805.00932.pdf)

* **Mobilenetv2: Inverted residuals and linear bottlenecks** (Sandler et al.; CVPR 2018) [[Paper]](https://arxiv.org/pdf/1801.04381.pdf)

* **Group Normalization** (Wu et al.; ECCV 2018) [[Paper]](https://arxiv.org/pdf/1803.08494.pdf)

* **Squeeze-and-Excitation Networks** （Hu et al.; CVPR 2018) [[Paper]](https://www.robots.ox.ac.uk/~vgg/publications/2018/Hu18/hu18.pdf)

* **Visualizing the Loss Landscape of Neural Nets** （Li et al. NIPS 2018) [[Paper]](https://papers.nips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf)

* **Gaussian Error Linear Units** (Hendrycks et al.; 2018; GELUs) [[Paper]](https://arxiv.org/pdf/1606.08415v3.pdf)
  * Article: [GELU activation](https://medium.com/@shoray.goel/gelu-gaussian-error-linear-unit-4ec59fb2e47c)
  * Related to Approximation of Gassuian error function:
    * **High Accurate Simple Approximation of Normal Distribution Integral** (Vazquez-Leal et al. 2012) [[URL]](https://www.hindawi.com/journals/mpe/2012/124029)
    * Book: J. H. Patel and C. B. Read, **Handbook of the Normal Distribution, Statistics A Series of Textbooks and Monographs**, Marcel Dekker, New York, NY, USA, 2nd edition, 1996.

* **Non-local neural networks** (Wang et al.; CVPR 2018) [[Paper]](https://arxiv.org/pdf/1711.07971.pdf)

### 2017

* **Aggregated Residual Transformations for Deep Neural Networks** （Xie et al.; CVPR 2017) [[Paper]](https://arxiv.org/pdf/1611.05431.pdf)

* **Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations** (Krueger et al.; ICLR 2017) [[Paper]](https://arxiv.org/pdf/1606.01305.pdf)

### 2016

* **Identity Mappings in Deep Residual Networks** (He et al.; ECCV 2016 camera-read) [[Paper]](https://arxiv.org/pdf/1603.05027.pdf)

* **Layer Normalization** (Ba, and E.Hinton, el al.; 2016) [[Paper]](https://arxiv.org/pdf/1607.06450.pdf)



### 2015

* **Deep Residual Learning for Image Recognition** (He et al. Cited by 71k) [[Paper]](https://arxiv.org/pdf/1512.03385.pdf)

* **Conditional Convolutional Neural Network for Modality-aware Face Recognition** (Xiong et al.; ICCV 2015) [[Paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xiong_Conditional_Convolutional_Neural_ICCV_2015_paper.pdf)


### 2010

* **Understanding the difficulty of training deep feedforward neural networks** (Glorot et al.; PMLR 2010; Cited by 11k) [[Paper]](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

## Articles

* [Why Relu? Tips for using Relu. Comparison between Relu, Leaky Relu, and Relu-6](https://medium.com/@chinesh4/why-relu-tips-for-using-relu-comparison-between-relu-leaky-relu-and-relu-6-969359e48310)

* [Batch Normalization, Instance Normalization, Layer Normalization: Structural Nuances](https://becominghuman.ai/all-about-normalization-6ea79e70894b)