

## Papers

### 2022

* **Self-Supervised Learning of Graph Neural Networks: A Unified Review** (Xie et al.; 2022) [[Paper]](https://arxiv.org/pdf/2102.10757.pdf)
### 2021

* **Characterizing signal propagation to close the performance gap in unnormalized resnets** (Brock et al.; ICLR 2021) [[Paper]](https://arxiv.org/pdf/2101.08692.pdf)

* **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale** (Dosovitskiy et al.; 2021; ViT: Vision Transformer) [[Paper]](https://arxiv.org/pdf/2010.11929.pdf)
  * [code and pretrained model](https://github.com/google-research/vision_transformer)
  
* **Revisiting ResNets: Improved Training and Scaling Strategies** (Bello et al.; 2021) [[Paper]](https://arxiv.org/pdf/2103.07579.pdf)

* **MLP-Mixer: An all-MLP Architecture for Vision** (Tolstikhin et al.; 2021; Google Brain) [[Paper]](https://arxiv.org/pdf/2105.01601.pdf)

* **Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet** (Kyriazi et al.; 2021; Oxford VGG) [[Paper]](https://arxiv.org/pdf/2105.02723.pdf)

* **Scaling Vision Transformers** (Zhai et al.; 2021) [[Paper]](https://arxiv.org/pdf/2106.04560v1.pdf) 

* **Multiscale Vision Transformers** (Fan et al.; 2021; FAIR) [[Paper]](https://arxiv.org/pdf/2104.11227.pdf)

* **Transformers in Vision: A Survey** (Khan et al.; 2021) [[Paper]](https://arxiv.org/pdf/2101.01169.pdf)

* **DeepViT: Towards Deeper Vision Transformer** (Zhou et al.; 2021; National University of Singapore and ByteDance US AI Lab) [[Paper]](https://arxiv.org/pdf/2103.11886.pdf)

* **How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers** (Steiner et al.; 2021) [[Paper]]()

* **SiT: Self-supervised vIsion Transformer** (Atito et al.; 2021) [[Paper]](https://arxiv.org/pdf/2104.03602.pdf)

* **How to decay your learning rate** （Lewkowycz et al.; 2021) [[Paper]](https://arxiv.org/pdf/2103.12682.pdf)

* [SEER: The start of a more powerful, flexible, and accessible era for computer vision](https://ai.facebook.com/blog/seer-the-start-of-a-more-powerful-flexible-and-accessible-era-for-computer-vision/) by Facebook
  * [Code](https://vissl.ai)

* **Perceiver: General Perception with Iterative Attention** (Jaegle et al.; ICML 2021; DeepMind) [[Paper]](https://arxiv.org/pdf/2103.03206.pdf)

* **YOLOX: Exceeding YOLO Series in 2021** (Zheng Ge et al.; Megvii Technology; 2021) [[Paper]](https://arxiv.org/pdf/2107.08430.pdf)

* **Activation Functions in Artificial Neural Networks: A Systematic Overview** (Lederer et al.; 2021) [[Paper]](https://arxiv.org/pdf/2101.09957.pdf)

* **The Principles of Deep Learning Theory - An Effective Theory Approach to Understanding Neural Networks** (Roberts et al.; 2021) [[Book]](https://arxiv.org/pdf/2106.10165.pdf)

### 2020

* **Meta pseudo labels** (Pham et al.; 2020) [[Paper]](https://arxiv.org/pdf/2003.10580.pdf)

* **Batch normalization biases residual blocks towards the identity function in deep networks** (De et al.; NeurPS 2020) [[Paper]](https://arxiv.org/pdf/2002.10444.pdf)

* **Circumventing Outliers of AutoAugment with Knowledge Distillation** (Wei et al.) [[Paper]](https://arxiv.org/pdf/2003.11342v1.pdf)

* **Self-training with noisy student improves imagenet classification** (Xie et al.; CVPR 2020) [[Paper]](https://arxiv.org/pdf/1911.04252.pdf)

* **Exploring self-attention for image recognition** (Zhao et al.; CVPR 2020) [[Paper]](https://arxiv.org/pdf/2004.13621.pdf)

* **Big Transfer (BiT): General Visual Representation Learning** (Kolesnikov et al.; 2020) [[Paper]](https://arxiv.org/pdf/1912.11370.pdf)

* **End-to-End Object Detection with Transformers** (Carion et al.; ECCV 2020; FAIR) [[Paper]](https://arxiv.org/pdf/2005.12872.pdf)

* **Quantifying attention flow in transformers** (Abnar et al.; 2020) [[Paper]](https://arxiv.org/pdf/2005.00928.pdf)

* **Training data-efficient image transformers & distillation through attention** (Touvron et al.; 2020) [[Paper]](https://arxiv.org/pdf/2012.12877.pdf)
  
* **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows** (Liu et al.; 2020; FAIR) [[Paper]](https://arxiv.org/pdf/2103.14030.pdf)
  * [Official code](https://github.com/microsoft/Swin-Transformer) on pytorch
  * [Tensorflow implementation](https://github.com/rishigami/Swin-Transformer-TF)
  
* **YOLOv4: Optimal Speed and Accuracy of Object Detection** (Bochkovskiy et al.; 2020) [[Paper]](https://arxiv.org/pdf/2004.10934.pdf)
  
### 2019

* **Micro-Batch Training with Batch-Channel Normalization and Weight Standardization** (Qiao et al.) [[Paper]](https://arxiv.org/pdf/1903.10520.pdf)

* **A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark** (Zhai et al.; 2019) [[Paper]](https://arxiv.org/pdf/1910.04867.pdf)

* **Data-efficient image recognition with contrastive predictive coding** (Hénaff et al.; ECCV 2020) [[Paper]](https://arxiv.org/pdf/1905.09272.pdf)

* **Searching for MobileNetV3** (Howard et al.; ICCV 2019) [[Paper]](https://arxiv.org/pdf/1905.02244.pdf)

* **Computing Receptive Fields of Convolutional Neural Networks** (Araujo et al.; Distill 2019) [[Paper]](https://distill.pub/2019/computing-receptive-fields/)

* **Explaining Explanations: An Overview of Interpretability of Machine Learning** (Gilpin et al.; DASS 2018) [[Paper]]()

### 2018

* **Exploring the limits of weakly supervised pretraining** (Mahajan, He, et al; ECCV 2018) [[Paper]](https://arxiv.org/pdf/1805.00932.pdf)

* **Mobilenetv2: Inverted residuals and linear bottlenecks** (Sandler et al.; CVPR 2018) [[Paper]](https://arxiv.org/pdf/1801.04381.pdf)

* **Group Normalization** (Wu et al.; ECCV 2018) [[Paper]](https://arxiv.org/pdf/1803.08494.pdf)

* **Squeeze-and-Excitation Networks** （Hu et al.; CVPR 2018) [[Paper]](https://www.robots.ox.ac.uk/~vgg/publications/2018/Hu18/hu18.pdf)

* **Visualizing the Loss Landscape of Neural Nets** （Li et al. NIPS 2018) [[Paper]](https://papers.nips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf)

* **Gaussian Error Linear Units** (Hendrycks et al.; 2018; GELUs) [[Paper]](https://arxiv.org/pdf/1606.08415v3.pdf) [[Paper]](https://arxiv.org/pdf/1606.08415.pdf)
  * Article: [GELU activation](https://medium.com/@shoray.goel/gelu-gaussian-error-linear-unit-4ec59fb2e47c)
  * Related to Approximation of Gassuian error function:
    * **High Accurate Simple Approximation of Normal Distribution Integral** (Vazquez-Leal et al. 2012) [[URL]](https://www.hindawi.com/journals/mpe/2012/124029)
    * Book: J. H. Patel and C. B. Read, **Handbook of the Normal Distribution, Statistics A Series of Textbooks and Monographs**, Marcel Dekker, New York, NY, USA, 2nd edition, 1996.

* **Non-local neural networks** (Wang et al.; CVPR 2018) [[Paper]](https://arxiv.org/pdf/1711.07971.pdf)

* **Mixed Precision Training** (Micikevicius et al.; ICLR 2018) [[Paper]](https://arxiv.org/pdf/1710.03740)
  
* **A guide to convolution arithmetic for deep learning** (Dumoulin et al. 2018) [[Paper]](https://arxiv.org/pdf/1603.07285.pdf)

* **YOLOv3: An Incremental Improvement** (Redmon et al.) [[Paper]](https://arxiv.org/pdf/1804.02767.pdf)

* **Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV).** (Kim et al.; PMLR 2018) [[Paper]](http://proceedings.mlr.press/v80/kim18d/kim18d.pdf)


### 2017

* **Aggregated Residual Transformations for Deep Neural Networks** （Xie et al.; CVPR 2017) [[Paper]](https://arxiv.org/pdf/1611.05431.pdf)

* **Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations** (Krueger et al.; ICLR 2017) [[Paper]](https://arxiv.org/pdf/1606.01305.pdf)

* **MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications** (Howard et al.; 2017) [[Paper]](https://arxiv.org/pdf/1704.04861.pdf)

* **Searching for Activation Functions** (Ramachandran et al.; Swish activation function) [[Paper]](https://arxiv.org/pdf/1710.05941.pdf)

* **Large Batch Training of Convolutional Networks** (You et al.; 2017) [[Technical Report]](https://arxiv.org/pdf/1708.03888.pdf)

### 2016

* **Identity Mappings in Deep Residual Networks** (He et al.; ECCV 2016 camera-read) [[Paper]](https://arxiv.org/pdf/1603.05027.pdf)

* **Layer Normalization** (Ba, and E.Hinton, el al.; 2016) [[Paper]](https://arxiv.org/pdf/1607.06450.pdf)

* **Understanding Convolutional Neural Networks** (Kousik et al.; NIPS 2016) [[Paper]](https://arxiv.org/pdf/1605.09081.pdf)

* **YOLO9000: Better, Faster, Stronger** (Redmon et al.; 2016) [[Paper]](https://arxiv.org/pdf/1612.08242.pdf)


### 2015

* **Deep Residual Learning for Image Recognition** (He et al. Cited by 71k) [[Paper]](https://arxiv.org/pdf/1512.03385.pdf)

* **Conditional Convolutional Neural Network for Modality-aware Face Recognition** (Xiong et al.; ICCV 2015) [[Paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xiong_Conditional_Convolutional_Neural_ICCV_2015_paper.pdf)

* **You Only Look Once: Unified, Real-Time Object Detection** （Redmon et al.; 2015) [[Paper]](https://arxiv.org/pdf/1506.02640.pdf)

* **Spatial Transformer Networks** (Jaderberg et al.; Google DeepMind; 2015) [[Paper]](


### 2014

* **How transferable are features in deep neural networks?** (Yosinski et al. NIPS 2014) [[Paper]](https://arxiv.org/pdf/1411.1792.pdf)
### 2010

* **Understanding the difficulty of training deep feedforward neural networks** (Glorot et al.; PMLR 2010; Cited by 11k) [[Paper]](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

* **Incorporating second-order functional knowledge for better option pricing** （Dugas et al.; NIPS 2000) [[Paper]](https://proceedings.neurips.cc/paper/2000/file/44968aece94f667e4095002d140b5896-Paper.pdf)

* **Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion** (Vincent et al.; JMLR 2010) [[Paper]](https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf)

### 2006

* **Reducing the Dimensionality of Data with Neural Networks** (Hinton et al.; Science 2006) [[Paper]](https://asset-pdf.scinapse.io/prod/2100495367/2100495367.pdf)
### 1991

* **Approximation capabilities of multilayer feedforward networks** (Hornik et al.; Neural Networks 1991) [[Paper]](http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf)
## Articles

* [Why Relu? Tips for using Relu. Comparison between Relu, Leaky Relu, and Relu-6](https://medium.com/@chinesh4/why-relu-tips-for-using-relu-comparison-between-relu-leaky-relu-and-relu-6-969359e48310)

* [Batch Normalization, Instance Normalization, Layer Normalization: Structural Nuances](https://becominghuman.ai/all-about-normalization-6ea79e70894b)

* [Interpreting your deep learning model by SHAP](https://towardsdatascience.com/interpreting-your-deep-learning-model-by-shap-e69be2b47893)

* [Computing Receptive Fields of Convolutional Neural Networks](https://distill.pub/2019/computing-receptive-fields/)

* [Some Loss Functions and Their Intuitive Explanations](https://jdhao.github.io/2017/03/13/some_loss_and_explanations/)
## Books

* [Dive into Deep Learning](https://d2l.ai)
