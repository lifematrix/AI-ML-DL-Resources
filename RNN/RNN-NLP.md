
## RNN

### Training Skills

* Dropout
  * [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)

### Attention

* Papers:
  * Bahdanau's [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
  
## NLP

### seq2seq

* Google's a general-purpose seq2seq(encoder-decoder) framework
  * [code in github](https://github.com/google/seq2seq)
  * [docs](https://google.github.io/seq2seq/)
  * paper
    * [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/abs/1703.03906)
    
* Tutorials
  * [A simple one](https://github.com/ematvey/tensorflow-seq2seq-tutorials) without using tf.contrib.seq2seq
  * [Minimum Seq2Seq implementation](https://gist.github.com/higepon/eb81ba0f6663a57ff1908442ce753084)
  * <span style="color:red">[Seq2Seq with Attention and Beam Search](https://guillaumegenthial.github.io/sequence-to-sequence.html)</span> 
  

### Neural Machine Translation
 
* Projects
  * [OpenNMT by Harvard NLP](http://opennmt.net)
  * Google's Neural Machine Translation Tutorials
    * [Traditional style using Tensorflow nightly](https://github.com/tensorflow/nmt)
    * [Google's tutorial using tensorflow 2.0](https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention) and keras style implementation.
  
* [Thang Luong's thesis on Neural Machine Translation](https://github.com/lmthang/thesis)

* Toturial
  * [Beam Search & Attention for text summarization made easy (Tutorial 5)](https://hackernoon.com/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086)